python -m llama_cpp.server --model ggml-model-q5_k.gguf --clip_model_path mmproj-model-f16.gguf --chat_format llava-1-5 --n_gpu_layers 1 --n_threads 8
